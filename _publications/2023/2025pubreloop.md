---

title:          "ReLoop: 'Seeing Twice and Thinking Backwards' via Closed-loop Training to Mitigate Hallucinations in Multimodal Understanding"
date:           2025-08-21 00:01:00 +0800
selected:       true
pub:            "EMNLP 2025 Findings (Accepted)"
pub_date:       "2025"
abstract: >-
  Multimodal Large Language Models (MLLMs) remain vulnerable to hallucinations that contradict or misrepresent
  input semantics. We propose ReLoop*, a closed-loop training framework that enforces cross-modal consistency
  via a ring-shaped structure with three complementary consistency feedback mechanisms. ReLoop encourages models
  to "see twice and think backwards", effectively mitigating hallucinations and improving semantic reliability
  in multimodal reasoning. Extensive experiments demonstrate significant improvements across multiple benchmarks.
cover:          /assets/images/photos/reloop.png
authors:
- "Jianjiang Yang* (first author)"
- Yanshu Li
- Ziyan Huang
links:
  Paper (arXiv): https://arxiv.org/abs/2507.04943



---
